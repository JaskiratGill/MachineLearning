{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE421 Lab 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeV9LVeRNNhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### PART 1 #####\n",
        "\n",
        "def MSE(W, b, x, y, reg):\n",
        "    # reshape y first\n",
        "    y_reshaped = np.transpose(y)\n",
        "\n",
        "    # blend the pixel data together per image\n",
        "    nr_images = x.shape[0]\n",
        "    x_reshaped = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2])) # original matrix remains untouched\n",
        "    x_reshaped = np.transpose(x_reshaped)  \n",
        "\n",
        "    # term1 = means squared error term\n",
        "    # term2 = weight decay error term\n",
        "    term1 = np.dot(np.transpose(W), x_reshaped) + b - y_reshaped # matrix multiplication of 2D matrices\n",
        "    term1 = np.dot(term1, np.transpose(term1)) / nr_images\n",
        "    term2 = (reg/2 * np.dot(np.transpose(W), W))\n",
        "    return (term1 + term2)\n",
        "\n",
        "def gradMSE(W, b, x, y, reg):\n",
        "    y_reshaped = np.transpose(y)\n",
        "    nr_images = x.shape[0]\n",
        "    x_reshaped = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2])) # original matrix remains untouched\n",
        "    x_reshaped = np.transpose(x_reshaped)\n",
        "\n",
        "    term1 = (np.dot(np.transpose(W), x_reshaped) + b - y_reshaped)\n",
        "    \n",
        "    grad_weight = 2/nr_images * np.matmul(x_reshaped, np.transpose(term1)) + reg*W\n",
        "    grad_bias = 2/nr_images * np.sum(term1)\n",
        "    \n",
        "    return grad_weight, grad_bias\n",
        "\n",
        "\n",
        "def plot_errors(d0, d1, d2, epochs, alpha, reg):\n",
        "    \n",
        "    print(f'plot_errors: plotting errors for alpha = {alpha}, reg = {reg}')\n",
        "\n",
        "    print(f'Max training error = {np.amax(d0)}, Min training error = {np.amin(d0)}')\n",
        "    print(f'Max val error = {np.amax(d1)}, Min val error = {np.amin(d1)}')\n",
        "    print(f'Max test error = {np.amax(d2)}, Min test error = {np.amin(d2)}')\n",
        "\n",
        "    plt.plot(np.arange(epochs), d0, label='Training error')\n",
        "    plt.plot(np.arange(epochs), d1, label='Validation error')\n",
        "    plt.plot(np.arange(epochs), d2, label='Testing error')\n",
        "    plt.title(f'Errors for model trained with alpha = {alpha}, reg = {reg}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol, val_data, val_labels, test_data, test_labels):\n",
        "    # Your implementation here\n",
        "\n",
        "    difference_weights = 0\n",
        "    train_error = []\n",
        "    val_error = []\n",
        "    test_error = []\n",
        "\n",
        "    for t in range(epochs):\n",
        "        grad_w, grad_b = gradMSE(W, b, x, y, reg)   \n",
        "        # full data is sent to gradMSE, means all pictures of 28*28\n",
        "        vt_w = -grad_w  # set direction for weights to move as negative of gradient\n",
        "        vt_b = -grad_b\n",
        "        W_new = W + alpha*vt_w  # update the weights\n",
        "        b_new = b + alpha*vt_b\n",
        "\n",
        "        single_train_err = MSE(W_new, b_new, x, y, reg)\n",
        "        single_val_err = MSE(W_new, b_new, val_data, val_labels, reg)\n",
        "        single_test_err = MSE(W_new, b_new, test_data, test_labels, reg)\n",
        "        train_error.append(single_train_err)\n",
        "\n",
        "        val_error.append(single_val_err)\n",
        "        test_error.append(single_test_err)\n",
        "\n",
        "        if (np.linalg.norm(W_new - W) < error_tol): # when difference is under error_tol, then done\n",
        "            W = W_new\n",
        "            b = b_new\n",
        "            train_error_array = np.array(train_error)\n",
        "            val_error_array = np.array(val_error)\n",
        "            test_error_array = np.array(test_error)\n",
        "            plot_errors(train_error_array, val_error_array, test_error_array, t+1, alpha, reg)\n",
        "            return W_new, b_new\n",
        "        else:\n",
        "            W = W_new\n",
        "            b = b_new\n",
        "\t\t\n",
        "    train_error_array = np.squeeze(np.array(train_error))\n",
        "    val_error_array = np.squeeze(np.array(val_error))\n",
        "    test_error_array = np.squeeze(np.array(test_error))\n",
        "    plot_errors(train_error_array, val_error_array, test_error_array, epochs, alpha, reg)\n",
        "    return W, b\t# these are updated final\n",
        "\n",
        "def leastSquares(x, y):\n",
        "    y_reshaped = np.transpose(y)\n",
        "    # blend the pixel data together per image\n",
        "    nr_images = x.shape[0]\n",
        "    x_reshaped = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2])) # original matrix remains untouched\n",
        "    x_pseudo = np.dot(np.linalg.inv(np.dot(np.transpose(x_reshaped), x_reshaped)), np.transpose(x_reshaped))\n",
        "    W_new = np.dot(x_pseudo, np.transpose(y_reshaped))\n",
        "\n",
        "    b_new = (1/nr_images)*np.sum(y-np.dot(x_reshaped, W_new))\n",
        "\n",
        "    return W_new, b_new\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5-YSxn0dcNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### PART 2 #####\n",
        "def crossEntropyLoss(W, b, x, y, reg):\n",
        "    # reshape x \n",
        "    x = x.reshape([x.shape[0], x.shape[1]*x.shape[2]])\n",
        "\n",
        "    N = y.shape[0]\n",
        "    \n",
        "    # calculate y_hat from formula given\n",
        "    y_hat = 1/(1 + np.exp(-(np.matmul(x,W) + b)))\n",
        "\n",
        "    # calculate cross entropy loss from formula given\n",
        "    LD = np.sum((-(y*np.log(y_hat))-((1-y)*np.log(1-y_hat)))/N)\n",
        "    LW = (reg/2)*(np.linalg.norm(W)**2)\n",
        "    cross_entropy_loss = LD + LW\n",
        "\n",
        "    return cross_entropy_loss\n",
        "\n",
        "\n",
        "def gradCE(W, b, x, y, reg):\n",
        "    # reshape x \n",
        "    x = x.reshape([x.shape[0], x.shape[1]*x.shape[2]])\n",
        "\n",
        "    x_transpose = x.transpose()\n",
        "    N = y.shape[0]\n",
        "\n",
        "    # calculate y_hat from formula given\n",
        "    y_hat = 1/(1 + np.exp(-(np.matmul(x,W) + b)))\n",
        "\n",
        "    gradient_w = np.matmul(x_transpose,(y_hat - y))/N + reg*W\n",
        "    gradient_b = np.sum((y_hat-y)/N)\n",
        "\n",
        "    return (gradient_w, gradient_b)\n",
        "\n",
        "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol, val_data, val_labels, test_data, test_labels, lossType):\n",
        "\n",
        "    train_error = []\n",
        "    val_error = []\n",
        "    test_error = []\n",
        "\n",
        "    train_accuracy = []\n",
        "    validity_accuracy = []\n",
        "    test_accuracy = []\n",
        "    \n",
        "    if lossType == \"MSE\":\n",
        "      for t in range(epochs):\n",
        "          grad_w, grad_b = gradMSE(W, b, x, y, reg)   \n",
        "\n",
        "          vt_w = -grad_w  \n",
        "          vt_b = -grad_b\n",
        "\n",
        "          W_new = W + alpha*vt_w  # update the weights\n",
        "          b_new = b + alpha*vt_b\n",
        "\n",
        "          single_train_err = MSE(W_new, b_new, x, y, reg)\n",
        "          single_val_err = MSE(W_new, b_new, val_data, val_labels, reg)\n",
        "          single_test_err = MSE(W_new, b_new, test_data, test_labels, reg)\n",
        "\n",
        "          train_error.append(single_train_err)\n",
        "          val_error.append(single_val_err)\n",
        "          test_error.append(single_test_err)\n",
        "\n",
        "          # when difference is under error_tol, then done\n",
        "          if (np.linalg.norm(W_new - W) < error_tol): \n",
        "              W = W_new\n",
        "              b = b_new\n",
        "              return W_new, b_new\n",
        "          else:\n",
        "              W = W_new\n",
        "              b = b_new\n",
        "      \n",
        "    elif lossType == \"CE\":\n",
        "      for t in range(epochs):\n",
        "\n",
        "          grad_w, grad_b = gradCE(W, b, x, y, reg)   \n",
        "\n",
        "          vt_w = -grad_w  \n",
        "          vt_b = -grad_b\n",
        "\n",
        "          W_new = W + alpha*vt_w  # update the weights\n",
        "          b_new = b + alpha*vt_b\n",
        "\n",
        "          single_train_err = crossEntropyLoss(W_new, b_new, x, y, reg)\n",
        "          single_val_err = crossEntropyLoss(W_new, b_new, val_data, val_labels, reg)\n",
        "          single_test_err = crossEntropyLoss(W_new, b_new, test_data, test_labels, reg)\n",
        "\n",
        "          train_error.append(single_train_err)\n",
        "          val_error.append(single_val_err)\n",
        "          test_error.append(single_test_err)\n",
        "\n",
        "          train_acc, val_acc, test_acc = accuracy(W, b, x, y, val_data, val_labels, test_data, test_labels)\n",
        "\n",
        "          train_accuracy.append(train_acc)\n",
        "          validity_accuracy.append(val_acc)\n",
        "          test_accuracy.append(test_acc)\n",
        "\n",
        "          # when difference is under error_tol, then done\n",
        "          if (np.linalg.norm(W_new - W) < error_tol): \n",
        "              W = W_new\n",
        "              b = b_new\n",
        "              train_error_array = np.array(train_error)\n",
        "              val_error_array = np.array(val_error)\n",
        "              test_error_array = np.array(test_error)\n",
        "              plot_errors(train_error_array, val_error_array, test_error_array, t+1, alpha, reg)\n",
        "              return W_new, b_new\n",
        "          else:\n",
        "              W = W_new\n",
        "              b = b_new\n",
        "\n",
        "    # plot the loss\n",
        "    plt.plot(train_error)\n",
        "    plt.plot(val_error)\n",
        "    plt.plot(test_error)\n",
        "    plt.suptitle('Logistic regression Loss: Alpha = %s lamba = %s' %(alpha,reg),fontsize=15)\n",
        "    plt.legend(['Training Loss','Validation Loss','Test Loss'])\n",
        "    plt.show()\n",
        "\n",
        "    # plot the accuracy\n",
        "    plt.plot(train_accuracy)\n",
        "    plt.plot(validity_accuracy)\n",
        "    plt.plot(test_accuracy)\n",
        "    plt.suptitle('Logistic regression Accuracy: Alpha = %s lamba = %s' %(alpha,reg),fontsize=15)\n",
        "    plt.legend(['Training Accuracy','Validation Accuracy','Test Accuracy'])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def accuracy(W, b, x, y, val_data, val_labels, test_data, test_labels):\n",
        "    \n",
        "    x = x.reshape([x.shape[0], x.shape[1]*x.shape[2]])\n",
        "    val_data = val_data.reshape([val_data.shape[0], val_data.shape[1]*val_data.shape[2]])\n",
        "    test_data = test_data.reshape([test_data.shape[0], test_data.shape[1]*test_data.shape[2]])\n",
        "\n",
        "    y_hat_train = 1/(1 + np.exp(-(np.matmul(x,W) + b)))\n",
        "    y_hat_val = 1/(1 + np.exp(-(np.matmul(val_data,W) + b)))\n",
        "    y_hat_test = 1/(1 + np.exp(-(np.matmul(test_data,W) + b)))\n",
        "\n",
        "    train_acc = np.sum((y_hat_train>=0.5)==y)/(x.shape[0])\n",
        "    val_acc = np.sum((y_hat_val>=0.5)==val_labels)/(val_labels.shape[0])\n",
        "    test_acc = np.sum((y_hat_test>=0.5)==test_labels)/(test_data.shape[0])\n",
        "\n",
        "    return train_acc, val_acc, test_acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PirlPiswScac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### PART 3 #####\n",
        "\n",
        "def accuracy (x,W,y):\n",
        "  W = W.eval()\n",
        "  y_hat = np.matmul(x,W)\n",
        "\n",
        "  for i in range(len(y_hat)):\n",
        "      if (y_hat[i][0])>=0.5: y_hat[i][0]=1\n",
        "      else: y_hat[i][0] = 0\n",
        "      \n",
        "  score = 0\n",
        "  for i in range(len(y_hat)):\n",
        "    if y_hat[i][0] == y[i][0]: score+=1\n",
        "\n",
        "  return score/len(y)\n",
        "\n",
        "def buildGraph(lossType=\"MSE\", batchSize= 500, betaOne=0.9, betaTwo=0.999, learning_rate=0.001, Epsilon= 1e-08):\n",
        "  #load the training, validation, and test data and targets\n",
        "  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "  beta = 0\n",
        "  tf.set_random_seed(421)\n",
        "\n",
        "  #Initialize weight and bias tensors\n",
        "  ShapeWVector = (trainData.shape[1]*trainData.shape[2])\n",
        "\n",
        "  W = tf.Variable(tf.truncated_normal(shape = (ShapeWVector, 1), mean = 0.0, stddev = 0.5, \n",
        "                                           dtype = tf.dtypes.float32, seed = None, name = \"weight\"))\n",
        "  bias = tf.zeros(shape=(1), dtype=tf.dtypes.float32)\n",
        "  X = tf.placeholder(dtype = tf.dtypes.float32, shape = (None, ShapeWVector), name = \"X\")\n",
        "  Y = tf.placeholder(dtype = tf.dtypes.float32, shape = (None, 1), name = \"Y\",)\n",
        "  myLambda = tf.placeholder(dtype = tf.dtypes.float32, shape = None, name = \"myLambda\")\n",
        "\n",
        "\n",
        "  if lossType == \"MSE\":\n",
        "    prediction = tf.matmul(X, W) + bias\n",
        "    loss = tf.losses.mean_squared_error(labels = Y, predictions = prediction)\n",
        "    regularizer = tf.nn.l2_loss(W)\n",
        "    loss += beta*regularizer\n",
        "\n",
        "  elif lossType == \"CE\":\n",
        "    logit = tf.matmul(X, W) + bias   \n",
        "    prediction = tf.sigmoid(logit)\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = Y, logits = prediction)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    regularizer = tf.nn.l2_loss(W)\n",
        "    loss += beta*regularizer \n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate, beta1= betaOne, beta2= betaTwo, epsilon= Epsilon).minimize(loss)\n",
        "  \n",
        "  #Either add SGD code here, or make a new function!\n",
        "\n",
        "  return W, bias, prediction, Y, X, loss, optimizer\n",
        "\n",
        "def SGD (lossType=\"MSE\", batchSize= 500, betaOne=0.9, betaTwo=0.999, learning_rate=0.001, Epsilon= 1e-08):\n",
        "\n",
        "  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "\n",
        "  weight, bias, prediction, y, x, loss, optimizer= buildGraph(lossType, batchSize, betaOne, betaTwo, learning_rate, Epsilon)\n",
        "\n",
        "  trainAccuracy = []\n",
        "  trainLoss = []\n",
        "\n",
        "  validAccuracy = []\n",
        "  validLoss = []\n",
        "\n",
        "  testAccuracy = []\n",
        "  testLoss = []\n",
        "\n",
        "  epochs = 700\n",
        "  numBatches = int(3500/batchSize)\n",
        "  validData = np.reshape(validData, (100, 784))\n",
        "  testData = np.reshape(testData, (145, 784))\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for j in range(epochs):\n",
        "\n",
        "      #Shuffling data starts\n",
        "      temp = np.c_[trainData.reshape(len(trainData), -1), trainTarget.reshape(len(trainTarget), -1)]\n",
        "      np.random.shuffle(temp)\n",
        "      trainData = temp[:, :trainData.size//len(trainData)].reshape(trainData.shape)\n",
        "      trainTarget = temp[:, trainData.size//len(trainData):].reshape(trainTarget.shape)\n",
        "      #shuffling data ends\n",
        "\n",
        "      for i in range(numBatches):\n",
        "        \n",
        "        data = trainData[i*batchSize: (i+1)*batchSize]\n",
        "        target = trainTarget[i*batchSize: (i+1)*batchSize]\n",
        "\n",
        "        data = np.reshape(data,(batchSize, 784))\n",
        "        target = np.reshape(target, (batchSize, 1))\n",
        "        sess.run(optimizer, feed_dict = {x:data, y:target})\n",
        "        \n",
        "      trainLoss.append(sess.run(loss, feed_dict = {x:data, y:target}))\n",
        "      validLoss.append(sess.run(loss, feed_dict = {x:validData, y:validTarget}))\n",
        "      testLoss.append(sess.run(loss, feed_dict = {x:testData, y:testTarget}))\n",
        "      \n",
        "      trainAccuracy.append(accuracy(data,weight,target))\n",
        "      validAccuracy.append(accuracy(validData,weight,validTarget))\n",
        "      testAccuracy.append(accuracy(testData,weight,testTarget))\n",
        "      \n",
        "    #different plotting scales  \n",
        "    if lossType == \"CE\":\n",
        "      LossYlim1 = 0.45\n",
        "      LossYlim2 = 0.8\n",
        "      AccuYlim1 = 0.5\n",
        "      AccuYlim2 = 1.05\n",
        "\n",
        "    elif lossType == \"MSE\":\n",
        "      LossYlim1 = 0\n",
        "      LossYlim2 = 18\n",
        "      AccuYlim1 = 0.2\n",
        "      AccuYlim2 = 1.0\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.plot(trainAccuracy)\n",
        "    plt.plot(validAccuracy)\n",
        "    plt.plot(testAccuracy)\n",
        "    plt.legend(['Train', 'Valid', 'Test'], loc='lower right')\n",
        "    plt.xlabel('Epoch', fontsize=16)\n",
        "    plt.ylabel('Accuracy', fontsize=16)\n",
        "    axes = plt.gca()\n",
        "    axes.set_ylim([AccuYlim1, AccuYlim2])\n",
        "    axes.set_xlim([-50,720])\n",
        "    plt.show()\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.plot(trainLoss)\n",
        "    plt.plot(validLoss)\n",
        "    plt.plot(testLoss)\n",
        "    plt.legend(['Train', 'Valid', 'Test'], loc='lower right')\n",
        "    plt.xlabel('Epoch', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    axes = plt.gca()\n",
        "    axes.set_ylim([LossYlim1,LossYlim2])\n",
        "    axes.set_xlim([-50,720])\n",
        "    plt.show()  \n",
        "\n",
        "    print(\"final training accuracy: \", trainAccuracy[-1])\n",
        "    print(\"final valid accuracy: \", validAccuracy[-1])\n",
        "    print(\"final test accuracy: \", testAccuracy[-1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}